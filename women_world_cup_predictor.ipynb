{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing and Predicting the Women's World Cup\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"#motivation\">Motivation</a></li>\n",
    "    <ul>\n",
    "    <li><a href=\"#requirements\">Requirements</a></li>\n",
    "    </ul>\n",
    "<li><a href=\"#wrangling\">Data Wrangling</a></li>\n",
    "<li><a href=\"#eda\">Exploratory Data Analysis</a></li>\n",
    "    <ul>\n",
    "    <li><a href=\"#question_1\">Item if needed</a></li>\n",
    "    </ul>\n",
    "<li><a href=\"#modeling\">Modeling</a></li>\n",
    "    <ul>\n",
    "    <li><a href=\"#preparation\">Data Preparation</a></li>\n",
    "    <li><a href=\"#preprocess\">Preprocessing</a></li>\n",
    "    <li><a href=\"#features\">Features</a></li>\n",
    "    <li><a href=\"#model_selection\">Model Selection</a></li>\n",
    "    </ul>\n",
    "<li><a href=\"#conclusions\">Conclusions</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='motivation'></a>\n",
    "## Motivation\n",
    "\n",
    "The FIFA Women's World Cup is in its eighth edition in 2019. It occurs every four years between June and July, and has teams from all continents. This edition is being held in France, and 24 teams qualified for the final tournament ([Wikipedia](https://en.wikipedia.org/wiki/2019_FIFA_Women%27s_World_Cup)).\n",
    "\n",
    "Besides the similarities, the women's football is not even close to have the same visibility as the men's one (at least not in Brazil, but we imagine that it's the same in the whole world), and founding the data about previous matches wasn't very easy. There wasn't data available on FIFA's website or in any other \"official\" provider, but we found it on [Kaggle](https://www.kaggle.com/alexkaechele/womens-world-cup) (thanks a lot for inputing this data by hand).\n",
    "\n",
    "This analysis and modeling has the intent of predicting the winners from the round of 16 to the final match. The data and code used is provided on our Github.\n",
    "\n",
    "We are very excited to know who is going to win, and we hope you enjoy the results as much as we did working on it.\n",
    "\n",
    "<a id='requirements'></a>\n",
    "### Requirements\n",
    "\n",
    "**python 3.7.3**\n",
    "\n",
    "* matplotlib==3.0.3\n",
    "* numpy==1.16.3\n",
    "* pandas==0.24.2\n",
    "* seaborn==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import plots as ps\n",
    "\n",
    "sns.set()\n",
    "sns.set_palette(\"GnBu_d\", 6)\n",
    "%matplotlib inline\n",
    "\n",
    "def create_link(id):\n",
    "    display(HTML(f'<a id={id}></a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_raw = pd.read_csv('womens_world_cup_data.csv')\n",
    "ranking = pd.read_csv('womens_world_cup_rankings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wrangling'></a>\n",
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_raw = pd.read_csv('womens_world_cup_data.csv')\n",
    "ranking = pd.read_csv('womens_world_cup_rankings.csv')\n",
    "\n",
    "display(scores_raw.shape)\n",
    "display(ranking.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranking.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The countries'names in `ranking` data have upper case letters, we are going to make it consistent with the `scores` data by changing them to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranking.team = ranking.team.apply(lambda x: x.lower())\n",
    "ranking.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the DataFrames\n",
    "\n",
    "In order to make easy to analyse the data, we are going to merge the DFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_matches_i_j = (scores_raw.merge(ranking, left_on='Team_i', right_on='team')\n",
    "                             .rename(columns={'Team_i': 'team_i',\n",
    "                                              'rating': 'rating_i',\n",
    "                                              'rank': 'rank_i'})\n",
    "                             .drop(columns=['team'])\n",
    "                             .merge(ranking, left_on='Team_j', right_on='team')\n",
    "                             .rename(columns={'Team_j': 'team_j',\n",
    "                                              'rating': 'rating_j',\n",
    "                                              'rank': 'rank_j'})\n",
    "                             .drop(columns=['team']))\n",
    "\n",
    "\n",
    "all_matches = all_matches_i_j.rename(columns={'team_i': 'team_a',\n",
    "                                              'home_i': 'home_a',\n",
    "                                              'score_i': 'score_a',\n",
    "                                              'rank_i': 'rank_a',\n",
    "                                              'rating_i': 'rating_a',\n",
    "                                              'team_j': 'team_b',\n",
    "                                              'home_j': 'home_b',\n",
    "                                              'score_j': 'score_b',\n",
    "                                              'rank_j': 'rank_b',\n",
    "                                              'rating_j': 'rating_b',})\n",
    "\n",
    "display(all_matches.head())\n",
    "display(all_matches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=''></a>\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "Here are some questions we are going to address in this section:\n",
    "\n",
    "[1. *Is a team more likely to win when playing at home?*](#question_1)\n",
    "\n",
    "[2. *How many matches happened per year?*](#question_2)\n",
    "\n",
    "[3. *Which are the teams with the most winnings?*](#question_3)\n",
    "\n",
    "[4. *Which are the teams with the most loss?*](#question_4)\n",
    "\n",
    "[5. *Higher ratings are related with more wins?*](#question_5)\n",
    "\n",
    "[6. *How are the scores distributions?*](#question_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(all_matches, figsize=(20,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some points:\n",
    "\n",
    "countries with more rating play more games. as we can see in `rating_i X rating_i` *(equal to rank_i, rating_j and rank_j)*\n",
    "\n",
    "There are more games at 2018.\n",
    "\n",
    "There are some correlations between scores of A and rating that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_matches.sample(2, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modeling'></a>\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Preparation\n",
    "\n",
    "**Guarantee that team I and team J respect lexical order**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that the `scores_raw` contains a row for every match, including the teams, if the team played at home, the scores, and the year. As the world cup is held in one country, we do not consider the data about playing in home relevant for this prediction, that's why we are going to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = scores_raw.drop(columns=['home_i', 'home_j'])\n",
    "display(scores.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores.loc[scores['Team_j'] < scores['Team_i'], ['Team_i', 'score_i', 'Team_j', 'score_j']] = \\\n",
    "    scores.loc[scores['Team_j'] < scores['Team_i'], ['Team_j', 'score_j', 'Team_i', 'score_i']].values\n",
    "        \n",
    "\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first model we want to predict which team wons the match.\n",
    "\n",
    "So, for that we are going to create a column target which says which one wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_matches['target'] = 'draw'\n",
    "all_matches.loc[all_matches.score_a > all_matches.score_b, ['target']] = 'win_a'\n",
    "all_matches.loc[all_matches.score_a < all_matches.score_b, ['target']] = 'win_b'\n",
    "print(f\"amount of target values: {all_matches['target'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in all_matches.columns:\n",
    "    print(f'{c} = {all_matches[c].unique()} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined ratings as continuous variable and all others are going to be categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_raw = all_matches.drop(columns=['target', 'score_a', 'score_b'])\n",
    "y_raw = all_matches['target']\n",
    "\n",
    "display(X_raw.shape)\n",
    "display(X_raw.sample(3, random_state=13))\n",
    "\n",
    "display(y_raw.shape)\n",
    "display(y_raw.sample(3, random_state=13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# enconding categorical features with one hot encoder\n",
    "feat_cats = (pd.get_dummies(X_raw[['team_a',\n",
    "                                   'team_b', 'year']].astype(str)))\n",
    "print('shape categorycal:')\n",
    "display(feat_cats.shape)\n",
    "\n",
    "# normalization numerical features\n",
    "from sklearn import preprocessing\n",
    "\n",
    "feat_nums_raw = preprocessing.scale(X_raw[['rating_a', 'rating_b']])\n",
    "feat_nums = pd.DataFrame(feat_nums_raw, columns=['rating_a', 'rating_b'])\n",
    "print('describe numerical:')\n",
    "display(feat_nums.describe())\n",
    "\n",
    "# merging data\n",
    "X = feat_nums.join(feat_cats)\n",
    "print('shape all merged:')\n",
    "display(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About Cross Validation\n",
    "\n",
    "We have to realize that we have few data points, because of that we are NOT follow the default data split:\n",
    "`Train | Cross Validation | Test`\n",
    "\n",
    "We are going to use kfold trying to not throw data away. We will define 5 buckets and then we are going to train each algorithm proposed 5 times, using k-1 bucket, and evaluate metric with the one that remains. In conclusion, we will have 5 metrics for each model, the final metric for each model is the average of those metrics. After that, we are going to compare each model to choose the best one.\n",
    "\n",
    "We will using Stratified Kfold to guarantee that classes are equally devided among the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "num_folds = 4\n",
    "kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=23)\n",
    "k_fold_indexes = [(train, test) for train, test in kf.split(X, y_raw)]\n",
    "\n",
    "labels=['win_a', 'draw', 'win_b']\n",
    "\n",
    "def train_model_k_fold(model, score, train_data=X, target_data=y_raw):\n",
    "    scores = []\n",
    "    confusion_matrixes = []\n",
    "    \n",
    "    for train, test in k_fold_indexes:       \n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_data.loc[train], train_data.loc[test], target_data.loc[train], target_data.loc[test]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        scores.append(model.score(X_test, y_test))\n",
    "        \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model - First Scene - Compare some algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "models = []\n",
    "\n",
    "models.append((\"Decision Tree\", DecisionTreeClassifier(random_state=4)))\n",
    "models.append((\"Logistic Regression\", LogisticRegression(solver='lbfgs', C=0.1, multi_class='auto', random_state=4)))\n",
    "models.append((\"SVC kernel=linear\", SVC(kernel='linear', random_state=4)))\n",
    "models.append((\"SVC kernel=poly\", SVC(kernel='poly', gamma='auto', random_state=23)))\n",
    "models.append((\"SVC kernel=rbf\", SVC(kernel='rbf', gamma='auto', random_state=4)))\n",
    "models.append((\"SVC kernel=sigmoid\", SVC(kernel='sigmoid', gamma='auto', random_state=4)))\n",
    "models.append((\"Random Forest\", RandomForestClassifier(n_estimators=10, random_state=4)))\n",
    "models.append((\"Gradient Boosting\", GradientBoostingClassifier(random_state=4)))\n",
    "\n",
    "\n",
    "for name, model in models:\n",
    "    print(f'{name}: {train_model_k_fold(model, accuracy_score)}')\n",
    "    ps.plot_learning_curve(model, name, X, y_raw, cv=k_fold_indexes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "`accuracy = 0.60`\n",
    "\n",
    "Learning Curve show us that model extremelly overfits the data, the gap between the curves show this, so the model has high variance and acquiring more data could help this overfits behavior. But, decision trees tend to overfit a lot, even more when we don't do feature selection, like we didn't.\n",
    "\n",
    "#### Logistic Regression\n",
    "`accuracy = 0.73`\n",
    "\n",
    "Learning Curve shows a very nice pattern: high score with close curves. We made a simple hyper parameter tunning at this stage (cheating, ok!) tunning C to regularize our 230+ features =D. Model answers very nice for our propose, one of the best choices.\n",
    "\n",
    "#### SVC\n",
    "`kernel: linear -> accuracy = 0.68`\n",
    "\n",
    "`kernel: poly -> accuracy = 0.65`\n",
    "\n",
    "`kernel: rbf -> accuracy = 0.72`\n",
    "\n",
    "`kernel: sigmoid -> accuracy = 0.72`\n",
    "\n",
    "At this point we made a simple hyper parameter tunning again (cheating), at my point of view, does no make sense choose an SVM model without choose a kernel, so we create 4 svc models, one for each basic kernel that has in sklearn.\n",
    "\n",
    "Linear kernel has high variance (overfit the data) and poly kernel has high bias (underfit the data). Sigmoid and rbf are both nice!\n",
    "\n",
    "#### Ensemble Methods\n",
    "`Gradient Boosting -> accuracy = 0.66`\n",
    "\n",
    "`Random Forest -> accuracy = 0.64`\n",
    "\n",
    "Both have good metrics, but is easy to see in learning curves that both are overfitted. Both cases, I think that more data could fix this overfitting problem, sadly we don't have more data, so let's move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Comments\n",
    "\n",
    "There are a bunch of other models that we could explore, but at this point, for our propose, those are enough to find a good solution.\n",
    "\n",
    "I decided to stress Linear Regression and SVC with sigmoid kernel to the next stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model - Second Scene - Stressing some algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this part we will find the best hyper parameters to chossed models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "scorer = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_lr = {'solver': ('newton-cg', 'sag', 'saga', 'lbfgs'),\n",
    "              'C': np.unique(np.geomspace(0.001, 1, num=15, dtype=float)),\n",
    "              'max_iter': np.unique(np.geomspace(50, 200, num=3, dtype=int))}\n",
    "\n",
    "lr = LogisticRegression(multi_class='auto')\n",
    "\n",
    "gd_model_lr = GridSearchCV(lr,\n",
    "                           parameters_lr,\n",
    "                           n_jobs=8,\n",
    "                           cv=k_fold_indexes,\n",
    "                           iid=True,\n",
    "                           scoring=scorer,\n",
    "                           verbose=1)\n",
    "gd_model_lr.fit(X, y_raw)\n",
    "\n",
    "display(gd_model_lr.best_estimator_)\n",
    "gd_model_lr.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_scv = {'kernel': ('sigmoid', 'rbf'),\n",
    "                  'C': np.unique(np.geomspace(0.001, 1, num=10, dtype=float)),\n",
    "                  'coef0': np.unique(np.geomspace(0.01, 10, num=5, dtype=float)),\n",
    "                  'class_weight': (None,\n",
    "                                   'balanced',\n",
    "                                   {'win_a': (354/(354-151)), 'win_b': (354/(354-151)), 'draw': (354/(354-52))}),\n",
    "                  'gamma': list(np.unique(np.geomspace(0.0001, 10, num=10, dtype=float))) + ['scale'],\n",
    "                  'tol': np.unique(np.geomspace(0.0001, 10, num=6, dtype=float))}\n",
    "\n",
    "\n",
    "svc = SVC(kernel='linear', decision_function_shape='ovo', random_state=4)\n",
    "\n",
    "gd_model_svc = GridSearchCV(svc,\n",
    "                            parameters_scv,\n",
    "                            n_jobs=8,\n",
    "                            cv=k_fold_indexes,\n",
    "                            iid=True,\n",
    "                            scoring=scorer,\n",
    "                            verbose=1)\n",
    "gd_model_svc.fit(X, y_raw)\n",
    "\n",
    "\n",
    "display(gd_model_svc.best_estimator_)\n",
    "gd_model_svc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gd_model_svc.best_estimator_\n",
    "print(f'Accuracy error: {train_model_k_fold(best_model, accuracy_score)}')\n",
    "ps.plot_learning_curve(best_model, 'Final Model - Learning Curve', X, y_raw, cv=k_fold_indexes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance based on decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels=['win_a', 'draw', 'win_b']\n",
    "cm1=confusion_matrix(y_test, pred, labels=labels)\n",
    "cm2=confusion_matrix(y_test, pred, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plots as ps\n",
    "\n",
    "from importlib import reload\n",
    "reload(ps)\n",
    "\n",
    "ps.print_confusion_matrixes([cm1, cm2, cm2, cm2, cm2], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
