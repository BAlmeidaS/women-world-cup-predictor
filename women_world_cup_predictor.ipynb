{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing and Predicting the Women's World Cup\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"#motivation\">Motivation</a></li>\n",
    "    <ul>\n",
    "    <li><a href=\"#requirements\">Requirements</a></li>\n",
    "    </ul>\n",
    "<li><a href=\"#wrangling\">Data Wrangling</a></li>\n",
    "<li><a href=\"#eda\">Exploratory Data Analysis</a></li>\n",
    "    <ul>\n",
    "    <li><a href=\"#question_1\">Item if needed</a></li>\n",
    "    </ul>\n",
    "<li><a href=\"#modeling\">Modeling</a></li>\n",
    "    <ul>\n",
    "    <li><a href=\"#preparation\">Data Preparation</a></li>\n",
    "    <li><a href=\"#preprocess\">Preprocessing</a></li>\n",
    "    <li><a href=\"#features\">Features</a></li>\n",
    "    <li><a href=\"#model_selection\">Model Selection</a></li>\n",
    "    </ul>\n",
    "<li><a href=\"#conclusions\">Conclusions</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='motivation'></a>\n",
    "## Motivation\n",
    "\n",
    "The FIFA Women's World Cup is in its eighth edition in 2019. It occurs every four years between June and July, and has teams from all continents. This edition is being held in France, and 24 teams qualified for the final tournament ([Wikipedia](https://en.wikipedia.org/wiki/2019_FIFA_Women%27s_World_Cup)).\n",
    "\n",
    "Besides the similarities, the women's football is not even close to have the same visibility as the men's one (at least not in Brazil, but we imagine that it's the same in the whole world), and founding the data about previous matches wasn't very easy. There wasn't data available on FIFA's website or in any other \"official\" provider, but we found it on [Kaggle](https://www.kaggle.com/alexkaechele/womens-world-cup) (thanks a lot for inputing this data by hand).\n",
    "\n",
    "This analysis and modeling has the intent of predicting the winners from the round of 16 to the final match. The data and code used is provided on our Github.\n",
    "\n",
    "We are very excited to know who is going to win, and we hope you enjoy the results as much as we did working on it.\n",
    "\n",
    "<a id='requirements'></a>\n",
    "### Requirements\n",
    "\n",
    "**python 3.7.3**\n",
    "\n",
    "* matplotlib==3.0.3\n",
    "* numpy==1.16.3\n",
    "* pandas==0.24.2\n",
    "* seaborn==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import plots as ps\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set()\n",
    "sns.set_palette(\"GnBu_d\", 6)\n",
    "%matplotlib inline\n",
    "\n",
    "def create_link(id):\n",
    "    display(HTML(f'<a id={id}></a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_raw = pd.read_csv('womens_world_cup_data.csv')\n",
    "ranking = pd.read_csv('womens_world_cup_rankings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wrangling'></a>\n",
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores_raw = pd.read_csv('womens_world_cup_data.csv')\n",
    "ranking = pd.read_csv('womens_world_cup_rankings.csv')\n",
    "\n",
    "display(scores_raw.shape)\n",
    "display(ranking.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranking.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The countries'names in `ranking` data have upper case letters, we are going to make it consistent with the `scores` data by changing them to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranking.team = ranking.team.apply(lambda x: x.lower())\n",
    "ranking.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the DataFrames\n",
    "\n",
    "In order to make easy to analyse the data, we are going to merge the DFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_matches_i_j = (scores_raw.merge(ranking, left_on='Team_i', right_on='team')\n",
    "                             .rename(columns={'Team_i': 'team_i',\n",
    "                                              'rating': 'rating_i',\n",
    "                                              'rank': 'rank_i'})\n",
    "                             .drop(columns=['team'])\n",
    "                             .merge(ranking, left_on='Team_j', right_on='team')\n",
    "                             .rename(columns={'Team_j': 'team_j',\n",
    "                                              'rating': 'rating_j',\n",
    "                                              'rank': 'rank_j'})\n",
    "                             .drop(columns=['team']))\n",
    "\n",
    "\n",
    "all_matches = all_matches_i_j.rename(columns={'team_i': 'team_a',\n",
    "                                              'home_i': 'home_a',\n",
    "                                              'score_i': 'score_a',\n",
    "                                              'rank_i': 'rank_a',\n",
    "                                              'rating_i': 'rating_a',\n",
    "                                              'team_j': 'team_b',\n",
    "                                              'home_j': 'home_b',\n",
    "                                              'score_j': 'score_b',\n",
    "                                              'rank_j': 'rank_b',\n",
    "                                              'rating_j': 'rating_b',})\n",
    "\n",
    "display(all_matches.head())\n",
    "display(all_matches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=''></a>\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "Here are some questions we are going to address in this section:\n",
    "\n",
    "[1. *Is a team more likely to win when playing at home?*](#question_1)\n",
    "\n",
    "[2. *How many matches happened per year?*](#question_2)\n",
    "\n",
    "[3. *Which are the teams with the most winnings?*](#question_3)\n",
    "\n",
    "[4. *Which are the teams with the most loss?*](#question_4)\n",
    "\n",
    "[5. *Higher ratings are related with more wins?*](#question_5)\n",
    "\n",
    "[6. *How are the scores distributions?*](#question_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(all_matches, figsize=(20,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some points:\n",
    "\n",
    "countries with more rating play more games. as we can see in `rating_i X rating_i` *(equal to rank_i, rating_j and rank_j)*\n",
    "\n",
    "There are more games at 2018.\n",
    "\n",
    "There are some correlations between scores of A and rating that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_matches.sample(2, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modeling'></a>\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Preparation\n",
    "\n",
    "**Guarantee that team I and team J respect lexical order**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that the `scores_raw` contains a row for every match, including the teams, if the team played at home, the scores, and the year. As the world cup is held in one country, we do not consider the data about playing in home relevant for this prediction, that's why we are going to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = all_matches.drop(columns=['home_a', 'home_b'])\n",
    "display(scores.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_teams(df, columns=['team', 'score', 'rank', 'rating']):\n",
    "    cols_a = [i + '_a' for i in columns] \n",
    "    cols_b = [i + '_b' for i in columns] \n",
    "    \n",
    "    df.loc[df['team_b'] < df['team_a'], cols_a + cols_b] = \\\n",
    "        df.loc[df['team_b'] < df['team_a'], cols_b + cols_a].values\n",
    "    \n",
    "    return df\n",
    "    \n",
    "order_teams(scores)\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the target value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Target distribution\n",
    "labels = ('win_a', 'draw', 'win_b')\n",
    "\n",
    "scores['target'] = labels[1]\n",
    "scores.loc[scores.score_a > scores.score_b, ['target']] = labels[0]\n",
    "scores.loc[scores.score_a < scores.score_b, ['target']] = labels[2]\n",
    "\n",
    "scores['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in scores.columns:\n",
    "    print(f'{c} = {scores[c].unique()} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined ratings as continuous variable and all others are going to be categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_raw = scores.drop(columns=['target', 'score_a', 'score_b'])\n",
    "y = scores['target']\n",
    "\n",
    "display(X_raw.shape)\n",
    "display(X_raw.sample(3, random_state=13))\n",
    "\n",
    "display(y.shape)\n",
    "display(y.sample(3, random_state=13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_countries = np.union1d(X_raw.team_a.unique(), X_raw.team_b.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enconding categorical features with one hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(categories=[all_countries, all_countries])\n",
    "\n",
    "feat_cats_raw = ohe.fit_transform(X_raw[['team_a', 'team_b']].astype(str))\n",
    "\n",
    "feat_cats = pd.DataFrame(feat_cats_raw.todense(), columns=ohe.get_feature_names()).astype(int)\n",
    "feat_cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization numerical features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "feat_nums_raw = scaler.fit_transform(X_raw[['rating_a', 'rating_b']])\n",
    "feat_nums = pd.DataFrame(feat_nums_raw, columns=['rating_a', 'rating_b'])\n",
    "display(feat_nums.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging data\n",
    "X = feat_nums.join(feat_cats)\n",
    "print(f'shape all merged: {X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About Cross Validation\n",
    "\n",
    "We have to realize that we have few data points, because of that we are NOT follow the default data split:\n",
    "`Train | Cross Validation | Test`\n",
    "\n",
    "We are going to use kfold trying to not throw data away. We will define 5 buckets and then we are going to train each algorithm proposed 5 times, using k-1 bucket, and evaluate metric with the one that remains. In conclusion, we will have 5 metrics for each model, the final metric for each model is the average of those metrics. After that, we are going to compare each model to choose the best one.\n",
    "\n",
    "We will using Stratified Kfold to guarantee that classes are equally devided among the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "num_folds = 4\n",
    "kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=43)\n",
    "kf.get_n_splits(X, y)\n",
    "k_fold_indexes = [(train, test) for train, test in kf.split(X, y)]\n",
    "\n",
    "def train_model_k_fold(model, score, train_data=X, target_data=y):\n",
    "    scores = []\n",
    "    confusion_matrixes = []\n",
    "    \n",
    "    for train, test in k_fold_indexes:       \n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_data.iloc[train], train_data.iloc[test], target_data.iloc[train], target_data.iloc[test]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        scores.append(score(model, X_test, y_test))\n",
    "        \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model - First Scene - Compare some algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have to choose a good metric to our model optimize. I found in [fbeta score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html) a good metric to our propose, more specific with a `Beta=2` with that model will target more in recall than in precision, this is good because we want to find each of the True Positives (for each class), explain more about this, we will calculate the True Positives for each class and then we will calculate an weighted average base on the ocurrency of each class.\n",
    "\n",
    "As we know, there is some unbalance between the 3 target class - draw, win_a and win_b - and probably draw will be sacrificed to we find better `win_a` and `win_b`. We are ok with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "score = make_scorer(fbeta_score, beta=2, labels=labels, average='weighted')\n",
    "# score = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "models = []\n",
    "\n",
    "models.append((\"Decision Tree\", DecisionTreeClassifier(random_state=4)))\n",
    "models.append((\"Logistic Regression\", LogisticRegression(solver='lbfgs', C=0.1, multi_class='auto', random_state=4)))\n",
    "models.append((\"SVC kernel=linear\", SVC(kernel='linear', random_state=4)))\n",
    "models.append((\"SVC kernel=poly\", SVC(kernel='poly', gamma='auto', random_state=23)))\n",
    "models.append((\"SVC kernel=rbf\", SVC(kernel='rbf', gamma='auto', random_state=4)))\n",
    "models.append((\"SVC kernel=sigmoid\", SVC(kernel='sigmoid', gamma='auto', random_state=4)))\n",
    "models.append((\"Random Forest\", RandomForestClassifier(n_estimators=10, random_state=4)))\n",
    "models.append((\"Gradient Boosting\", GradientBoostingClassifier(random_state=4)))\n",
    "\n",
    "\n",
    "for name, model in models:\n",
    "    print(f'{name}: {train_model_k_fold(model, score)}')\n",
    "    ps.plot_learning_curve(model, name, X, y, cv=k_fold_indexes, scoring=score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Decision Tree\n",
    "`f2-score = 0.63`\n",
    "\n",
    "Learning Curve show us that model extremelly overfits the data, the gap between the curves show this, so the model has high variance and acquiring more data could help this overfits behavior. But, decision trees tend to overfit a lot, even more when we don't do feature selection, like we didn't.\n",
    "\n",
    "#### Logistic Regression\n",
    "`f2-score = 0.69`\n",
    "\n",
    "Learning Curve shows a very nice pattern: high score with some close curves. We made a simple hyper parameter tunning at this stage (cheating, ok!) tunning C to regularize our 230+ features =D. Model answers very nice for our propose, one of the best choices.\n",
    "\n",
    "#### SVC\n",
    "`kernel: linear -> f2-score = 0.64`\n",
    "\n",
    "`kernel: poly -> f2-score = 0.38`\n",
    "\n",
    "`kernel: rbf -> f2-score = 0.67`\n",
    "\n",
    "`kernel: sigmoid -> f2-score = 0.67`\n",
    "\n",
    "At this point we made a simple hyper parameter tunning again (cheating), at my point of view, does no make sense choose an SVM model without choose a kernel, so we create 4 svc models, one for each basic kernel that has in sklearn.\n",
    "\n",
    "Linear kernel has high variance (overfit the data) and poly kernel has high bias (underfit the data). Sigmoid and rbf are both nice!\n",
    "\n",
    "#### Ensemble Methods\n",
    "`Gradient Boosting -> f2-score = 0.63`\n",
    "\n",
    "`Random Forest -> f2-score = 0.64`\n",
    "\n",
    "Both have good metrics, but is easy to see in learning curves that both are overfitted. Both cases, I think that more data could fix this overfitting problem, sadly we don't have more data, so let's move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Final Comments\n",
    "\n",
    "There are a bunch of other models that we could explore, but at this point, for our propose, those are enough to find a good solution.\n",
    "\n",
    "I decided to stress Linear Regression and SVC with sigmoid and rbf kernel to the next stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model - Second Scene - Stressing some algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this part we will find the best hyper parameters to chossed models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "parameters_lr = {'solver': ('newton-cg', 'sag', 'saga', 'lbfgs'),\n",
    "                 'C': np.unique(np.geomspace(0.001, 1, num=15, dtype=float)),\n",
    "                 'max_iter': np.unique(np.geomspace(50, 200, num=3, dtype=int)),\n",
    "                 'class_weight': (None,\n",
    "                                  'balanced',\n",
    "                                  {'win_a': (354/(354-151)), 'win_b': (354/(354-151)), 'draw': (354/(354-52))},\n",
    "                                  {'win_a': (354/(354-151)), 'win_b': (354/(354-151)), 'draw': (354/(354-(2*52)))})}\n",
    "\n",
    "lr = LogisticRegression(multi_class='auto')\n",
    "\n",
    "gd_model_lr = GridSearchCV(lr,\n",
    "                           parameters_lr,\n",
    "                           n_jobs=8,\n",
    "                           cv=k_fold_indexes,\n",
    "                           iid=True,\n",
    "                           scoring=score,\n",
    "                           verbose=1)\n",
    "gd_model_lr.fit(X, y)\n",
    "\n",
    "display(gd_model_lr.best_estimator_)\n",
    "gd_model_lr.best_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_scv = {'kernel': ('sigmoid', 'rbf'),\n",
    "                  'C': np.unique(np.geomspace(0.01, 1000, num=10, dtype=float)),\n",
    "                  'coef0': np.unique(np.geomspace(0.01, 10, num=5, dtype=float)),\n",
    "                  'class_weight': (None,\n",
    "                                   'balanced',\n",
    "                                   {'win_a': (354/(354-151)), 'win_b': (354/(354-151)), 'draw': (354/(354-52))},\n",
    "                                   {'win_a': (354/(354-151)), 'win_b': (354/(354-151)), 'draw': (354/(354-(2*52)))}),\n",
    "                  'gamma': list(np.unique(np.geomspace(0.0001, 10, num=7, dtype=float))) + ['scale', 'auto'],\n",
    "                  'tol': np.unique(np.geomspace(0.0001, 10, num=6, dtype=float))}\n",
    "\n",
    "\n",
    "svc = SVC(decision_function_shape='ovo', random_state=37)\n",
    "\n",
    "gd_model_svc = GridSearchCV(svc,\n",
    "                            parameters_scv,\n",
    "                            n_jobs=8,\n",
    "                            cv=k_fold_indexes,\n",
    "                            iid=True,\n",
    "                            scoring=score,\n",
    "                            verbose=1)\n",
    "gd_model_svc.fit(X, y)\n",
    "\n",
    "\n",
    "display(gd_model_svc.best_estimator_)\n",
    "gd_model_svc.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model - Last Scene - The Final One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "best_model = gd_model_svc.best_estimator_\n",
    "\n",
    "print('The final Model with the hyper parameters is:')\n",
    "display(best_model)\n",
    "\n",
    "print(f'The fbeta score of it is {train_model_k_fold(best_model, score)}')\n",
    "\n",
    "ps.plot_learning_curve(best_model, 'Final Model - Learning Curve', X, y, cv=k_fold_indexes, scoring=score);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrixes = []\n",
    "    \n",
    "for train, test in k_fold_indexes:       \n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test]\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "    cm = confusion_matrix(y_test, best_model.predict(X_test), labels=labels)\n",
    "    confusion_matrixes.append(cm)\n",
    "    \n",
    "ps.print_confusion_matrixes(confusion_matrixes, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have a test data to make a last evaluation of the model, because, like we sad, we going to a k-fold approach. At this point we just want to see the confusion matrix of the model, but to see this we have to plot each confusion matrix for each model created in k-fold function that we designed. The plots are above.\n",
    "\n",
    "We could see that the weakness of out model is the power of predict draws. The recall and precision of each win class (win_a and win_b) are good and we are happy with that. In fact we are going to use the model to predict knockout phase, so a model that does not predict a draw is very useful at this point.\n",
    "\n",
    "Overall we are satisfied with the model and the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the rest of the Female World Cup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict we will retrain the model found with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = best_model.fit(X, y)\n",
    "\n",
    "def preprocess(df_raw):    \n",
    "    df = (df_raw.merge(ranking, left_on='team_a', right_on='team')\n",
    "                .rename(columns={'rating': 'rating_a',\n",
    "                                 'rank': 'rank_a'})\n",
    "                .drop(columns=['team'])\n",
    "                .merge(ranking, left_on='team_b', right_on='team')\n",
    "                .rename(columns={'rating': 'rating_b',\n",
    "                                 'rank': 'rank_b'})\n",
    "                .drop(columns=['team']))\n",
    "    \n",
    "    df = order_teams(df, columns=['team', 'rank', 'rating'])\n",
    "    \n",
    "    feat_nums_raw = scaler.transform(df[['rating_a', 'rating_b']])\n",
    "    feat_nums = pd.DataFrame(feat_nums_raw, columns=['rating_a', 'rating_b'])\n",
    "\n",
    "    feat_cats_raw = ohe.transform(df[['team_a', 'team_b']].astype(str))\n",
    "    feat_cats = pd.DataFrame(feat_cats_raw.todense(), columns=ohe.get_feature_names()).astype(int)\n",
    "    \n",
    "    return feat_nums.join(feat_cats), df\n",
    "\n",
    "def display_results(round_list):\n",
    "    r = {'team_a': [a for a, _ in round_list],\n",
    "         'team_b': [b for _, b in round_list],}\n",
    "\n",
    "    df = pd.DataFrame.from_dict(r)\n",
    "    \n",
    "    preprocessed, preds = preprocess(df)\n",
    "    preds['prediction'] = final_model.predict(preprocessed)\n",
    "    preds['winner'] = preds.team_a\n",
    "    preds.loc[preds['prediction'] == 'win_b', ['winner']] = preds.loc[preds['prediction'] == 'win_b', ['team_b']].values\n",
    "    display(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_16 = [('norway', 'australia'),\n",
    "            ('england', 'cameroon'),\n",
    "            ('france', 'brazil'),\n",
    "            ('spain', 'united states'),\n",
    "            ('italy', 'china'),\n",
    "            ('netherlands', 'japan'),\n",
    "            ('germany', 'nigeria'),\n",
    "            ('sweden', 'canada'),] \n",
    "\n",
    "display_results(round_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_8 = [('australia', 'england'),\n",
    "           ('france', 'united states'),\n",
    "           ('china', 'netherlands'),\n",
    "           ('germany', 'sweden')] \n",
    "\n",
    "display_results(round_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_4 = [('england', 'united states'),\n",
    "           ('netherlands', 'sweden'),] \n",
    "\n",
    "display_results(round_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_place = [('england', 'netherlands'),] \n",
    "\n",
    "display_results(third_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = [('united states', 'sweden'),] \n",
    "\n",
    "display_results(final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
