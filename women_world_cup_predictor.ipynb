{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing and Predicting the Women's World Cup\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"#motivation\">Motivation</a></li>\n",
    "    <ul>\n",
    "    <li><a href=\"#requirements\">Requirements</a></li>\n",
    "    </ul>\n",
    "<li><a href=\"#wrangling\">Data Wrangling</a></li>\n",
    "<li><a href=\"#eda\">Exploratory Data Analysis</a></li>\n",
    "    <ul>\n",
    "    <li><a href=\"#question_1\">Item if needed</a></li>\n",
    "    </ul>\n",
    "<li><a href=\"#modeling\">Modeling</a></li>\n",
    "    <ul>\n",
    "    <li><a href=\"#preparation\">Data Preparation</a></li>\n",
    "    <li><a href=\"#preprocess\">Preprocessing</a></li>\n",
    "    <li><a href=\"#features\">Features</a></li>\n",
    "    <li><a href=\"#model_selection\">Model Selection</a></li>\n",
    "    </ul>\n",
    "<li><a href=\"#conclusions\">Conclusions</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='motivation'></a>\n",
    "## Motivation\n",
    "\n",
    "The FIFA Women's World Cup is in its eighth edition in 2019. It occurs every four years between June and July, and has teams from all continents. This edition is being held in France, and 24 teams qualified for the final tournament ([Wikipedia](https://en.wikipedia.org/wiki/2019_FIFA_Women%27s_World_Cup)).\n",
    "\n",
    "Besides the similarities, the women's football is not even close to have the same visibility as the men's one (at least not in Brazil, but we imagine that it's the same in the whole world), and founding the data about previous matches wasn't very easy. There wasn't data available on FIFA's website or in any other \"official\" provider, but we found it on [Kaggle](https://www.kaggle.com/alexkaechele/womens-world-cup) (thanks a lot for inputing this data by hand).\n",
    "\n",
    "This analysis and modeling has the intent of predicting the winners from the round of 16 to the final match. The data and code used is provided on our Github.\n",
    "\n",
    "We are very excited to know who is going to win, and we hope you enjoy the results as much as we did working on it.\n",
    "\n",
    "<a id='requirements'></a>\n",
    "### Requirements\n",
    "\n",
    "**python 3.7.3**\n",
    "\n",
    "* matplotlib==3.0.3\n",
    "* numpy==1.16.3\n",
    "* pandas==0.24.2\n",
    "* seaborn==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import plots as ps\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set()\n",
    "sns.set_palette(\"GnBu_d\", 6)\n",
    "%matplotlib inline\n",
    "\n",
    "def create_link(id):\n",
    "    display(HTML(f'<a id={id}></a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_raw = pd.read_csv('womens_world_cup_data.csv')\n",
    "ranking = pd.read_csv('womens_world_cup_rankings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wrangling'></a>\n",
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores_raw = pd.read_csv('womens_world_cup_data.csv')\n",
    "ranking = pd.read_csv('womens_world_cup_rankings.csv')\n",
    "\n",
    "display(scores_raw.shape)\n",
    "display(ranking.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranking.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The countries'names in `ranking` data have upper case letters, we are going to make it consistent with the `scores` data by changing them to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranking.team = ranking.team.apply(lambda x: x.lower())\n",
    "ranking.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the DataFrames\n",
    "\n",
    "In order to make easy to analyse the data, we are going to merge the DFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_matches_i_j = (scores_raw.merge(ranking, left_on='Team_i', right_on='team')\n",
    "                             .rename(columns={'Team_i': 'team_i',\n",
    "                                              'rating': 'rating_i',\n",
    "                                              'rank': 'rank_i'})\n",
    "                             .drop(columns=['team'])\n",
    "                             .merge(ranking, left_on='Team_j', right_on='team')\n",
    "                             .rename(columns={'Team_j': 'team_j',\n",
    "                                              'rating': 'rating_j',\n",
    "                                              'rank': 'rank_j'})\n",
    "                             .drop(columns=['team']))\n",
    "\n",
    "\n",
    "all_matches = all_matches_i_j.rename(columns={'team_i': 'team_a',\n",
    "                                              'home_i': 'home_a',\n",
    "                                              'score_i': 'score_a',\n",
    "                                              'rank_i': 'rank_a',\n",
    "                                              'rating_i': 'rating_a',\n",
    "                                              'team_j': 'team_b',\n",
    "                                              'home_j': 'home_b',\n",
    "                                              'score_j': 'score_b',\n",
    "                                              'rank_j': 'rank_b',\n",
    "                                              'rating_j': 'rating_b',})\n",
    "\n",
    "display(all_matches.head())\n",
    "display(all_matches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=''></a>\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "Here are some questions we are going to address in this section:\n",
    "\n",
    "[1. *Is a team more likely to win when playing at home?*](#question_1)\n",
    "\n",
    "[2. *How many matches happened per year?*](#question_2)\n",
    "\n",
    "[3. *Which are the teams with the most winnings?*](#question_3)\n",
    "\n",
    "[4. *Which are the teams with the most loss?*](#question_4)\n",
    "\n",
    "[5. *Higher ratings are related with more wins?*](#question_5)\n",
    "\n",
    "[6. *How are the scores distributions?*](#question_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(all_matches, figsize=(20,20));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some points:\n",
    "\n",
    "countries with more rating play more games. as we can see in `rating_i X rating_i` *(equal to rank_i, rating_j and rank_j)*\n",
    "\n",
    "There are more games at 2018.\n",
    "\n",
    "There are some correlations between scores of A and rating that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_matches.sample(2, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modeling'></a>\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to start recognizing that we have few data points, really few! 354 data points precisely. We are going to try modelling some DS model with really few data points. Because of that, we want to remove some features, the first think is to remove the `home feature` which says if a team is playing at home. This data is kind of useless to predict world cup also, cause in this case, only France is in home. Doing this, one thing is very important to do: **Guarantee that team I and team J respect lexical order** and also their features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = all_matches.drop(columns=['home_a', 'home_b'])\n",
    "display(scores.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_teams(df, columns=['team', 'score', 'rank', 'rating']):\n",
    "    cols_a = [i + '_a' for i in columns] \n",
    "    cols_b = [i + '_b' for i in columns] \n",
    "    \n",
    "    df.loc[df['team_b'] < df['team_a'], cols_a + cols_b] = \\\n",
    "        df.loc[df['team_b'] < df['team_a'], cols_b + cols_a].values\n",
    "    \n",
    "    return df\n",
    "    \n",
    "order_teams(scores)\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to create a target value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = ('win_a', 'draw', 'win_b')\n",
    "\n",
    "scores['target'] = labels[1]\n",
    "scores.loc[scores.score_a > scores.score_b, ['target']] = labels[0]\n",
    "scores.loc[scores.score_a < scores.score_b, ['target']] = labels[2]\n",
    "\n",
    "scores['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we have to stop a moment and analysing each feature distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for c in scores.columns:\n",
    "    print(f'{c} = {scores[c].unique()} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined ratings as continuous variable and all others are going to be categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_raw = scores.drop(columns=['target', 'score_a', 'score_b'])\n",
    "y = scores['target']\n",
    "\n",
    "display(X_raw.shape)\n",
    "display(X_raw.sample(3, random_state=13))\n",
    "\n",
    "display(y.shape)\n",
    "display(y.sample(3, random_state=13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we decide to not use the year feature. The year column guides the model to a path of a [time-series problem](https://en.wikipedia.org/wiki/Time_series) and with lack of data we do not want walk to this way =]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [one hot enconding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/) to categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enconding categorical features with one hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(categories=[all_countries, all_countries])\n",
    "\n",
    "feat_cats_raw = ohe.fit_transform(X_raw[['team_a', 'team_b']].astype(str))\n",
    "\n",
    "feat_cats = pd.DataFrame(feat_cats_raw.todense(), columns=ohe.get_feature_names()).astype(int)\n",
    "feat_cats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a default preprocess data step, we will standardization numerical features. [reference](https://sebastianraschka.com/Articles/2014_about_feature_scaling.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "feat_nums_raw = scaler.fit_transform(X_raw[['rating_a', 'rating_b']])\n",
    "feat_nums = pd.DataFrame(feat_nums_raw, columns=['rating_a', 'rating_b'])\n",
    "display(feat_nums.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging data\n",
    "X = feat_nums.join(feat_cats)\n",
    "print(f'shape all merged: {X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About Cross Validation\n",
    "\n",
    "Again at this point, we have to say that we have few data points, because of that we are NOT follow the default data split:\n",
    "`Train | Cross Validation | Test`\n",
    "\n",
    "We are going to use [kfold](https://scikit-learn.org/stable/modules/cross_validation.html#k-fold) trying to not throw data away. We will define 4 buckets and then we are going to train each algorithm proposed45 times, using k-1 bucket, and evaluate metric with the one that remains. In conclusion, we will have 4 metrics for each model, the final metric for each model is the average of those metrics. After that, we are going to compare each model to choose the best one.\n",
    "\n",
    "We will using Stratified Kfold to guarantee that classes are equally devided among the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "num_folds = 4\n",
    "kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=43)\n",
    "kf.get_n_splits(X, y)\n",
    "k_fold_indexes = [(train, test) for train, test in kf.split(X, y)]\n",
    "\n",
    "def train_model_k_fold(model, score, train_data=X, target_data=y):\n",
    "    scores = []\n",
    "    confusion_matrixes = []\n",
    "    \n",
    "    for train, test in k_fold_indexes:       \n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_data.iloc[train], train_data.iloc[test], target_data.iloc[train], target_data.iloc[test]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        scores.append(score(model, X_test, y_test))\n",
    "        \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model - First Scene - Compare some algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to choose a good metric to our model optimizing. I found in [fbeta score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html) a good metric to our propose, more specific with a `Beta=2` with that model will target more in recall than in precision, this is good because we want to find each of the True Positives (for each class). Explaining more about this, we will calculate the True Positives for each class and then we will calculate an weighted average base on the ocurrency of each class.\n",
    "\n",
    "As we know, there is some unbalance between the 3 target class - draw, win_a and win_b - and probably draw will be sacrificed to we find better `win_a` and `win_b`. We are ok with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "score = make_scorer(fbeta_score, beta=2, labels=labels, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we want to compare models with our data and see how they fit with it. The propose at this stage is to define the simplest model (without hyperparameter tunning) for the algorithm that we are considering. So we make some analysis trying to find if the model has high variance, high bias, how it fits the data.\n",
    "\n",
    "I will plot [learning curves](https://www.ritchieng.com/machinelearning-learning-curve/) to make those analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "models = []\n",
    "\n",
    "models.append((\"Decision Tree\", DecisionTreeClassifier(random_state=4)))\n",
    "models.append((\"Logistic Regression\", LogisticRegression(solver='lbfgs', C=0.1, multi_class='auto', random_state=4)))\n",
    "models.append((\"SVC kernel=linear\", SVC(kernel='linear', random_state=4)))\n",
    "models.append((\"SVC kernel=poly\", SVC(kernel='poly', gamma='auto', random_state=23)))\n",
    "models.append((\"SVC kernel=rbf\", SVC(kernel='rbf', gamma='auto', random_state=4)))\n",
    "models.append((\"SVC kernel=sigmoid\", SVC(kernel='sigmoid', gamma='auto', random_state=4)))\n",
    "models.append((\"Random Forest\", RandomForestClassifier(n_estimators=10, random_state=4)))\n",
    "models.append((\"Gradient Boosting\", GradientBoostingClassifier(random_state=4)))\n",
    "\n",
    "\n",
    "for name, model in models:\n",
    "    print(f'{name}: {train_model_k_fold(model, score)}')\n",
    "    ps.plot_learning_curve(model, name, X, y, cv=k_fold_indexes, scoring=score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "`f2-score = 0.59`\n",
    "\n",
    "Learning Curve show us that model extremelly overfits the data, the gap between the curves show this, so the model has high variance and acquiring more data could help this overfits behavior. But, decision trees tend to overfit a lot, even more when we don't do feature selection, like we didn't.\n",
    "\n",
    "#### Logistic Regression\n",
    "`f2-score = 0.69`\n",
    "\n",
    "Learning Curve shows a very nice pattern: high score with some close curves. We made a simple hyper parameter tunning at this stage (cheating, ok!) tunning C to regularize our 230+ features =D. Model answers very nice for our propose, one of the best choices.\n",
    "\n",
    "#### SVC\n",
    "`kernel: linear -> f2-score = 0.67`\n",
    "\n",
    "`kernel: poly -> f2-score = 0.50`\n",
    "\n",
    "`kernel: rbf -> f2-score = 0.68`\n",
    "\n",
    "`kernel: sigmoid -> f2-score = 0.68`\n",
    "\n",
    "At this point we made a simple hyper parameter tunning again (cheating), at my point of view, does no make sense choose an SVM model without choose a kernel, so we create 4 svc models, one for each basic kernel that has in sklearn.\n",
    "\n",
    "Linear kernel has high variance (overfit the data) and poly kernel has high bias (underfit the data). Sigmoid and rbf are both nice!\n",
    "\n",
    "#### Ensemble Methods\n",
    "`Gradient Boosting -> f2-score = 0.64`\n",
    "\n",
    "`Random Forest -> f2-score = 0.63`\n",
    "\n",
    "Both have good metrics, but is easy to see in learning curves that both are overfitted. Both cases, I think that more data could fix this overfitting problem, sadly we don't have more data, so let's move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Final Comments\n",
    "\n",
    "There are a bunch of other models that we could explore, but at this point, for our propose, those are enough to find a good solution.\n",
    "\n",
    "I decided to stress Linear Regression and SVC with sigmoid and rbf kernel to the next stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model - Second Scene - Stressing some algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this part we will find the best hyper parameters to chossed models. We will use the simplest way to find the best hyper parameters a [grid search](https://towardsdatascience.com/grid-search-for-model-tuning-3319b259367e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_lr = {'solver': ('newton-cg', 'sag', 'saga', 'lbfgs'),\n",
    "                 'C': np.unique(np.geomspace(0.001, 1, num=15, dtype=float)),\n",
    "                 'max_iter': np.unique(np.geomspace(50, 200, num=3, dtype=int)),\n",
    "                 'class_weight': (None,\n",
    "                                  'balanced',\n",
    "                                  {'win_a': (354/(354-151)), 'win_b': (354/(354-151)), 'draw': (354/(354-52))},\n",
    "                                  {'win_a': (354/(354-151)), 'win_b': (354/(354-151)), 'draw': (354/(354-(2*52)))})}\n",
    "\n",
    "lr = LogisticRegression(multi_class='auto')\n",
    "\n",
    "gd_model_lr = GridSearchCV(lr,\n",
    "                           parameters_lr,\n",
    "                           n_jobs=8,\n",
    "                           cv=k_fold_indexes,\n",
    "                           iid=True,\n",
    "                           scoring=score,\n",
    "                           verbose=1)\n",
    "gd_model_lr.fit(X, y)\n",
    "\n",
    "display(gd_model_lr.best_estimator_)\n",
    "gd_model_lr.best_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_scv = {'kernel': ('sigmoid', 'rbf'),\n",
    "                  'C': np.unique(np.geomspace(0.01, 1000, num=10, dtype=float)),\n",
    "                  'coef0': np.unique(np.geomspace(0.01, 10, num=5, dtype=float)),\n",
    "                  'class_weight': (None,\n",
    "                                   'balanced',\n",
    "                                   {'win_a': (354/(354-151)), 'win_b': (354/(354-151)), 'draw': (354/(354-52))},\n",
    "                                   {'win_a': (354/(354-151)), 'win_b': (354/(354-151)), 'draw': (354/(354-(2*52)))}),\n",
    "                  'gamma': list(np.unique(np.geomspace(0.0001, 10, num=7, dtype=float))) + ['scale', 'auto'],\n",
    "                  'tol': np.unique(np.geomspace(0.0001, 10, num=6, dtype=float))}\n",
    "\n",
    "\n",
    "svc = SVC(decision_function_shape='ovo', random_state=37)\n",
    "\n",
    "gd_model_svc = GridSearchCV(svc,\n",
    "                            parameters_scv,\n",
    "                            n_jobs=8,\n",
    "                            cv=k_fold_indexes,\n",
    "                            iid=True,\n",
    "                            scoring=score,\n",
    "                            verbose=1)\n",
    "gd_model_svc.fit(X, y)\n",
    "\n",
    "\n",
    "display(gd_model_svc.best_estimator_)\n",
    "gd_model_svc.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model - Last Scene - The Final One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "best_model = gd_model_svc.best_estimator_\n",
    "\n",
    "print('The final Model with the hyper parameters is:')\n",
    "display(best_model)\n",
    "\n",
    "print(f'The fbeta score of it is {train_model_k_fold(best_model, score)}')\n",
    "\n",
    "ps.plot_learning_curve(best_model, 'Final Model - Learning Curve', X, y, cv=k_fold_indexes, scoring=score);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrixes = []\n",
    "    \n",
    "for train, test in k_fold_indexes:       \n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test]\n",
    "\n",
    "    best_model.fit(X_train, y_train)\n",
    "    cm = confusion_matrix(y_test, best_model.predict(X_test), labels=labels)\n",
    "    confusion_matrixes.append(cm)\n",
    "    \n",
    "ps.print_confusion_matrixes(confusion_matrixes, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have a test data to make a last evaluation of the model, because, like we said, we going to a k-fold approach. At this point we just want to see the confusion matrix of \"the best model\". However to see this we have to plot each confusion matrix for each model created in k-fold function that we designed. The plots are above.\n",
    "\n",
    "We could see that the weakness of out model is the power of predict draws. The recall and precision of each win class (win_a and win_b) are good and we are happy with that. A little parenthesis in here, the precision of first model represented by the k=0 confusion matrix, has a little low precision of win_b, we already expected that because we choosed `Beta=2` in our score, but this is particulary lower than we expected, but we are ok and decide to move on. \n",
    "\n",
    "In fact we are going to use the model to predict knockout phase, so a model that does not predict a draw is very useful at this point.\n",
    "\n",
    "Overall we are satisfied with the model and the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the rest of the Female World Cup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict we will retrain the model found with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = best_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then create some helpful functions to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df_raw):    \n",
    "    df = (df_raw.merge(ranking, left_on='team_a', right_on='team')\n",
    "                .rename(columns={'rating': 'rating_a',\n",
    "                                 'rank': 'rank_a'})\n",
    "                .drop(columns=['team'])\n",
    "                .merge(ranking, left_on='team_b', right_on='team')\n",
    "                .rename(columns={'rating': 'rating_b',\n",
    "                                 'rank': 'rank_b'})\n",
    "                .drop(columns=['team']))\n",
    "    \n",
    "    df = order_teams(df, columns=['team', 'rank', 'rating'])\n",
    "    \n",
    "    feat_nums_raw = scaler.transform(df[['rating_a', 'rating_b']])\n",
    "    feat_nums = pd.DataFrame(feat_nums_raw, columns=['rating_a', 'rating_b'])\n",
    "\n",
    "    feat_cats_raw = ohe.transform(df[['team_a', 'team_b']].astype(str))\n",
    "    feat_cats = pd.DataFrame(feat_cats_raw.todense(), columns=ohe.get_feature_names()).astype(int)\n",
    "    \n",
    "    return feat_nums.join(feat_cats), df\n",
    "\n",
    "def display_results(round_list):\n",
    "    r = {'team_a': [a for a, _ in round_list],\n",
    "         'team_b': [b for _, b in round_list],}\n",
    "\n",
    "    df = pd.DataFrame.from_dict(r)\n",
    "    \n",
    "    preprocessed, preds = preprocess(df)\n",
    "    preds['prediction'] = final_model.predict(preprocessed)\n",
    "    preds['winner'] = preds.team_a\n",
    "    preds.loc[preds['prediction'] == 'win_b', ['winner']] = preds.loc[preds['prediction'] == 'win_b', ['team_b']].values\n",
    "    display(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally - The predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_16 = [('norway', 'australia'),\n",
    "            ('england', 'cameroon'),\n",
    "            ('france', 'brazil'),\n",
    "            ('spain', 'united states'),\n",
    "            ('italy', 'china'),\n",
    "            ('netherlands', 'japan'),\n",
    "            ('germany', 'nigeria'),\n",
    "            ('sweden', 'canada'),] \n",
    "\n",
    "display_results(round_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_8 = [('australia', 'england'),\n",
    "           ('france', 'united states'),\n",
    "           ('china', 'netherlands'),\n",
    "           ('germany', 'sweden')] \n",
    "\n",
    "display_results(round_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_4 = [('england', 'united states'),\n",
    "           ('netherlands', 'sweden'),] \n",
    "\n",
    "display_results(round_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = [('united states', 'sweden'),] \n",
    "\n",
    "display_results(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_place = [('england', 'netherlands'),] \n",
    "\n",
    "display_results(third_place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strange Cenario of our model\n",
    "\n",
    "Lets suppose that france win the game against united states, the cenario will be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_4 = [('england', 'france'),\n",
    "           ('netherlands', 'sweden'),] \n",
    "\n",
    "display_results(round_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = [('france', 'sweden'),] \n",
    "\n",
    "display_results(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We particulary loved this cenario! According with our model: if France wins United States on round 8, the big underdogs Sweden wins the World Cup!!!!\n",
    "\n",
    "We are cheering for Sweden!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
