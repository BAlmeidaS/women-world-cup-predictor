{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing and Predicting the Women's World Cup\n",
    "\n",
    "<ul>\n",
    "<li><a href=\"#motivation\">Motivation</a></li>\n",
    "    <ul>\n",
    "    <li><a href=\"#requirements\">Requirements</a></li>\n",
    "    </ul>\n",
    "<li><a href=\"#wrangling\">Data Wrangling</a></li>\n",
    "<li><a href=\"#eda\">Exploratory Data Analysis</a></li>\n",
    "<li><a href=\"#modeling\">Modeling</a></li>\n",
    "    <ul>\n",
    "    <li><a href=\"#preparation\">Data Preparation</a></li>\n",
    "    <li><a href=\"#preprocess\">Preprocessing</a></li>\n",
    "    <li><a href=\"#features\">Features</a></li>\n",
    "    <li><a href=\"#model_selection\">Model Selection</a></li>\n",
    "    </ul>\n",
    "<li><a href=\"#conclusions\">Conclusions</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='motivation'></a>\n",
    "## Motivation\n",
    "\n",
    "The FIFA Women's World Cup is in its eighth edition in 2019. It occurs every four years between June and July, and has teams from all continents. This edition is being held in France, and 24 teams qualified for the final tournament ([Wikipedia](https://en.wikipedia.org/wiki/2019_FIFA_Women%27s_World_Cup)).\n",
    "\n",
    "Besides the similarities, the women's football is not even close to have the same visibility as the men's one (at least not in Brazil, but we imagine that it's the same in the whole world), and founding the data about previous matches wasn't very easy. There wasn't data available on FIFA's website or in any other \"official\" provider, but we found it on [Kaggle](https://www.kaggle.com/alexkaechele/womens-world-cup) (thanks a lot for inputing this data by hand).\n",
    "\n",
    "This analysis and modeling has the intent of predicting the winners from the round of 16 to the final match. The data and code used is provided on our Github.\n",
    "\n",
    "We are very excited to know who is going to win, and we hope you enjoy the results as much as we did working on it.\n",
    "\n",
    "<a id='requirements'></a>\n",
    "### Requirements\n",
    "\n",
    "**python 3.7.3**\n",
    "\n",
    "* matplotlib==3.0.3\n",
    "* numpy==1.16.3\n",
    "* pandas==0.24.2\n",
    "* seaborn==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import plots as ps\n",
    "\n",
    "output_notebook()\n",
    "sns.set()\n",
    "sns.set_palette(\"BuGn_r\", 6)\n",
    "%matplotlib inline\n",
    "\n",
    "def create_link(id):\n",
    "    display(HTML(f'<a id={id}></a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_raw = pd.read_csv('womens_world_cup_data.csv')\n",
    "ranking = pd.read_csv('womens_world_cup_rankings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wrangling'></a>\n",
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores_raw = pd.read_csv('womens_world_cup_data.csv')\n",
    "ranking = pd.read_csv('womens_world_cup_rankings.csv')\n",
    "\n",
    "display(scores_raw.shape)\n",
    "display(ranking.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranking.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The countries'names in `ranking` data have upper case letters, we are going to make it consistent with the `scores` data by changing them to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranking.team = ranking.team.apply(lambda x: x.lower())\n",
    "ranking.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the DataFrames\n",
    "\n",
    "In order to make easy to analyse the data, we are going to merge the DFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_matches_i_j = (scores_raw.merge(ranking, left_on='Team_i', right_on='team')\n",
    "                             .rename(columns={'Team_i': 'team_i',\n",
    "                                              'rating': 'rating_i',\n",
    "                                              'rank': 'rank_i'})\n",
    "                             .drop(columns=['team'])\n",
    "                             .merge(ranking, left_on='Team_j', right_on='team')\n",
    "                             .rename(columns={'Team_j': 'team_j',\n",
    "                                              'rating': 'rating_j',\n",
    "                                              'rank': 'rank_j'})\n",
    "                             .drop(columns=['team']))\n",
    "\n",
    "\n",
    "all_matches = all_matches_i_j.rename(columns={'team_i': 'team_a',\n",
    "                                              'home_i': 'home_a',\n",
    "                                              'score_i': 'score_a',\n",
    "                                              'rank_i': 'rank_a',\n",
    "                                              'rating_i': 'rating_a',\n",
    "                                              'team_j': 'team_b',\n",
    "                                              'home_j': 'home_b',\n",
    "                                              'score_j': 'score_b',\n",
    "                                              'rank_j': 'rank_b',\n",
    "                                              'rating_j': 'rating_b',})\n",
    "\n",
    "display(all_matches.head())\n",
    "display(all_matches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eda'></a>\n",
    "## Exploratory Data Analysis\n",
    "\n",
    "Here are some questions we are going to address in this section:\n",
    "\n",
    "[1. *Is a team more likely to win when playing at home?*](#question_1)\n",
    "\n",
    "[2. *How many matches happened per year?*](#question_2)\n",
    "\n",
    "[3. *Which are the top 10 teams with the most winnings?*](#question_3)\n",
    "\n",
    "[4. *Which are the top 10 teams with the most loss?*](#question_4)\n",
    "\n",
    "[5. *Higher ratings are related to more wins?*](#question_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just keeping the merged data unalterated\n",
    "eda_matches = all_matches.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_matches['winner'] = 'draw'\n",
    "eda_matches.loc[eda_matches.score_a > eda_matches.score_b, ['winner']] = 'win_a'\n",
    "eda_matches.loc[eda_matches.score_a < eda_matches.score_b, ['winner']] = 'win_b'\n",
    "eda_matches.sample(2, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's check how many draws and winnings we have in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = eda_matches.groupby('winner').count()[['team_a']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bar_chart(xlabels, counts, title):\n",
    "    result = xlabels\n",
    "    counts = counts\n",
    "\n",
    "    p = figure(x_range=result, plot_height=250, title=title,\n",
    "               toolbar_location=None, tools=\"\")\n",
    "\n",
    "    p.vbar(x=result, top=counts, width=0.9, color='#6baed6')\n",
    "\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.y_range.start = 0\n",
    "    if (len(xlabels) > 6):\n",
    "        p.xaxis.major_label_orientation = 0.75\n",
    "    show(p);\n",
    "    \n",
    "def plot_scatter(df_x, df_y, title):\n",
    "\n",
    "    plt.scatter(df_x, df_y, alpha=0.5, color='#6baed6')\n",
    "    plt.xlabel(df_x.columns[0], fontsize=14)\n",
    "    plt.ylabel(df_y.columns[0], fontsize=14)\n",
    "    plt.title(title, fontsize=18)\n",
    "    plt.tick_params(axis='both', labelsize=12)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_bar_chart(winners.index.values, winners.team_a, 'Results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question_1'></a>\n",
    "### 1. Is a team more likely to win when playing at home?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_a = eda_matches[['team_a', 'home_a', 'winner']].rename(columns={'team_a': 'team', 'home_a': 'home'})\n",
    "matches_b = eda_matches[['team_b', 'home_b', 'winner']].rename(columns={'team_b': 'team', 'home_b': 'home'})\n",
    "\n",
    "matches_a['team'] = 'team_a'\n",
    "matches_b['team'] = 'team_b'\n",
    "\n",
    "eda_all_matches = pd.concat([matches_a, matches_b], sort=False)\n",
    "eda_all_matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of matches at home\n",
    "at_home = eda_all_matches[eda_all_matches['home'] == 1]\n",
    "\n",
    "#number of matches out of home\n",
    "not_home = eda_all_matches[eda_all_matches['home'] == 0]\n",
    "\n",
    "# proportion of matches at home\n",
    "p_home = at_home.shape[0] / eda_all_matches.shape[0]\n",
    "\n",
    "# proportion of matches out of home\n",
    "p_not_home = not_home.shape[0] / eda_all_matches.shape[0]\n",
    "\n",
    "print(f'Proportion of matches at home: {p_home}')\n",
    "print(f'Proportion of matches not at home: {p_not_home}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of teams that won at home\n",
    "won_home = (at_home.query('team == \"team_a\" and winner == \"win_a\"').shape[0] +\n",
    "            at_home.query('team == \"team_b\" and winner == \"win_b\"').shape[0])\n",
    "p_win_home = won_home / at_home.shape[0]\n",
    "print(f'Proportion of teams that won at home: {p_win_home}')\n",
    "\n",
    "# proportion of teams that lost at home\n",
    "lost_home = (at_home.query('team == \"team_a\" and winner == \"win_b\"').shape[0] +\n",
    "             at_home.query('team == \"team_b\" and winner == \"win_a\"').shape[0])\n",
    "p_lose_home = lost_home / at_home.shape[0]\n",
    "print(f'Proportion of teams that lost at home: {p_lose_home}')\n",
    "\n",
    "# proportion of team that draw at home\n",
    "p_draw_home = at_home.query('winner == \"draw\"').shape[0] / at_home.shape[0]\n",
    "print(f'Proportion of teams that draw at home: {p_draw_home}')\n",
    "\n",
    "# proportion of teams that won out of home\n",
    "won_out_home = (not_home.query('team == \"team_a\" and winner == \"win_a\"').shape[0] +\n",
    "                not_home.query('team == \"team_b\" and winner == \"win_b\"').shape[0])\n",
    "p_win_out_home = won_out_home / not_home.shape[0]\n",
    "print(f'Proportion of teams that won out of home: {p_win_out_home}')\n",
    "\n",
    "# proportion of teams that lost out of home\n",
    "lost_out_home = (not_home.query('team == \"team_a\" and winner == \"win_b\"').shape[0] +\n",
    "                 not_home.query('team == \"team_b\" and winner == \"win_a\"').shape[0])\n",
    "p_lose_out_home = lost_out_home / not_home.shape[0]\n",
    "print(f'Proportion of teams that lost out of home: {p_lose_out_home}')\n",
    "\n",
    "# proportion of team that draw out of home\n",
    "p_draw_out_home = not_home.query('winner == \"draw\"').shape[0] / not_home.shape[0]\n",
    "print(f'Proportion of teams that draw out of home: {p_draw_out_home}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to answer if a team is more likely to win if it's playing at home, we need to know the probability of winning, as follow below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability of winning\n",
    "p_win = (p_home * p_win_home) + (p_not_home * p_win_out_home)\n",
    "print(f'Probability of winning: {p_win}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we can see the probability of winning is 42.6%, while the probability of winning if playing at home is 47.4%.**\n",
    "\n",
    "<a id='question_2'></a>\n",
    "### 2. How many matches happened per year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_per_year = eda_matches.groupby('year').count()[['team_a']]\n",
    "print_bar_chart(matches_per_year.index.values.astype(str), matches_per_year.team_a, 'Matches per Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question_3'></a>\n",
    "### 3. Which are the top 10 teams with the most winnings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_a = eda_matches.query('winner == \"win_a\"')[['team_a', 'rating_a']].rename(columns={'team_a': 'team',\n",
    "                                                                                         'rating_a': 'victories'})\n",
    "teams_b = eda_matches.query('winner == \"win_b\"')[['team_b', 'rating_b']].rename(columns={'team_b': 'team',\n",
    "                                                                                         'rating_b': 'victories'})\n",
    "\n",
    "winning_teams = pd.concat([teams_a, teams_b])\n",
    "winning_teams = winning_teams.groupby('team').count().sort_values(by=['victories'], ascending=False)\n",
    "top_10_win = winning_teams.head(10)\n",
    "\n",
    "print_bar_chart(top_10_win.index.values, top_10_win.victories, 'Top 10 teams with most Victories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question_4'></a>\n",
    "### 4. Which are the top 10 teams with the most losses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_a = eda_matches.query('winner == \"win_b\"')[['team_a', 'rating_a']].rename(columns={'team_a': 'team',\n",
    "                                                                                         'rating_a': 'losses'})\n",
    "teams_b = eda_matches.query('winner == \"win_a\"')[['team_b', 'rating_b']].rename(columns={'team_b': 'team',\n",
    "                                                                                         'rating_b': 'losses'})\n",
    "\n",
    "lossing_teams = pd.concat([teams_a, teams_b])\n",
    "lossing_teams = lossing_teams.groupby('team').count().sort_values(by=['losses'], ascending=False)\n",
    "top_10_loss = lossing_teams.head(10)\n",
    "\n",
    "print_bar_chart(top_10_loss.index.values, top_10_loss.losses, 'Top 10 teams with most Losses')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='question_5'></a>\n",
    "### 5. Are higher ratings related to more games?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_a = eda_matches[['team_a', 'rating_a', 'winner']].rename(columns={'team_a': 'team', 'rating_a': 'rating',\n",
    "                                                                                            'winner': 'matches'})\n",
    "matches_b = eda_matches[['team_b', 'rating_b', 'winner']].rename(columns={'team_b': 'team', 'rating_b': 'rating',\n",
    "                                                                                            'winner': 'matches'})\n",
    "\n",
    "matches_rating = pd.concat([matches_a, matches_b], sort=False)\n",
    "matches_count = matches_rating.groupby('team').count()[['matches']]\n",
    "matches_rating.drop(columns=['matches'], inplace=True)\n",
    "matches_rating.drop_duplicates(inplace=True)\n",
    "matches_rating.set_index('team', inplace=True)\n",
    "matches_rating = matches_rating.join(matches_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_rating.plot('rating', 'matches', kind='scatter')\n",
    "plot_scatter(matches_rating[['rating']], matches_rating[['matches']], 'Rating x Matches played')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart above shows us that there's a relationship between the rating and the number of games played. Teams with a greater rating played more games, which makes sense, since the data we gathered is from previous championships, and the teams that managed to keep further in the competitions were the best ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modeling'></a>\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Preparation\n",
    "\n",
    "**Guarantee that team I and team J respect lexical order**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that the `scores_raw` contains a row for every match, including the teams, if the team played at home, the scores, and the year. As the world cup is held in one country, we do not consider the data about playing in home relevant for this prediction, that's why we are going to drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = scores_raw.drop(columns=['home_i', 'home_j'])\n",
    "display(scores.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores.loc[scores['Team_j'] < scores['Team_i'], ['Team_i', 'score_i', 'Team_j', 'score_j']] = \\\n",
    "    scores.loc[scores['Team_j'] < scores['Team_i'], ['Team_j', 'score_j', 'Team_i', 'score_i']].values\n",
    "        \n",
    "\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first model we want to predict which team wons the match.\n",
    "\n",
    "So, for that we are going to create a column target which says which one wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_matches['target'] = 'draw'\n",
    "all_matches.loc[all_matches.score_a > all_matches.score_b, ['target']] = 'win_a'\n",
    "all_matches.loc[all_matches.score_a < all_matches.score_b, ['target']] = 'win_b'\n",
    "print(f\"amount of target values: {all_matches['target'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in all_matches.columns:\n",
    "    print(f'{c} = {all_matches[c].unique()} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined ratings as continuous variable and all others are going to be categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_raw = all_matches.drop(columns=['target', 'score_a', 'score_b'])\n",
    "y_raw = all_matches['target']\n",
    "\n",
    "display(X_raw.shape)\n",
    "display(X_raw.sample(3, random_state=13))\n",
    "\n",
    "display(y_raw.shape)\n",
    "display(y_raw.sample(3, random_state=13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# enconding categorical features with one hot encoder\n",
    "feat_cats = (pd.get_dummies(X_raw[['team_a',\n",
    "                                   'team_b', 'year']].astype(str)))\n",
    "print('shape categorycal:')\n",
    "display(feat_cats.shape)\n",
    "\n",
    "# normalization numerical features\n",
    "from sklearn import preprocessing\n",
    "\n",
    "feat_nums_raw = preprocessing.scale(X_raw[['rating_a', 'rating_b']])\n",
    "feat_nums = pd.DataFrame(feat_nums_raw, columns=['rating_a', 'rating_b'])\n",
    "print('describe numerical:')\n",
    "display(feat_nums.describe())\n",
    "\n",
    "# merging data\n",
    "X = feat_nums.join(feat_cats)\n",
    "print('shape all merged:')\n",
    "display(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### About Cross Validation\n",
    "\n",
    "We have to realize that we have few data points, because of that we are NOT follow the default data split:\n",
    "`Train | Cross Validation | Test`\n",
    "\n",
    "We are going to use kfold trying to not throw data away. We will define 5 buckets and then we are going to train each algorithm proposed 5 times, using k-1 bucket, and evaluate metric with the one that remains. In conclusion, we will have 5 metrics for each model, the final metric for each model is the average of those metrics. After that, we are going to compare each model to choose the best one.\n",
    "\n",
    "We will using Stratified Kfold to guarantee that classes are equally devided among the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "num_folds = 5\n",
    "kf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=6)\n",
    "k_fold_indexes = [(train, test) for train, test in kf.split(X, y_raw)]\n",
    "\n",
    "labels=['win_a', 'draw', 'win_b']\n",
    "\n",
    "def train_model_k_fold(model, score, train_data=X, target_data=y_raw):\n",
    "    scores = []\n",
    "    confusion_matrixes = []\n",
    "    \n",
    "    for train, test in k_fold_indexes:       \n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            train_data.loc[train], train_data.loc[test], target_data.loc[train], target_data.loc[test]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        scores.append(model.score(X_test, y_test))\n",
    "        \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model - First Scene - Compare some algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "models = []\n",
    "\n",
    "models.append((\"Decision Tree\", DecisionTreeClassifier(random_state=4)))\n",
    "models.append((\"Logistic Regression\", LogisticRegression(solver='lbfgs', C=0.1, multi_class='auto', random_state=4)))\n",
    "models.append((\"SVC kernel=linear\", SVC(kernel='linear', random_state=4)))\n",
    "models.append((\"SVC kernel=poly\", SVC(kernel='poly', gamma='auto', random_state=23)))\n",
    "models.append((\"SVC kernel=rbf\", SVC(kernel='rbf', gamma='auto', random_state=4)))\n",
    "models.append((\"SVC kernel=sigmoid\", SVC(kernel='sigmoid', gamma='auto', random_state=4)))\n",
    "models.append((\"Random Forest\", RandomForestClassifier(n_estimators=10, random_state=4)))\n",
    "models.append((\"Gradient Boosting\", GradientBoostingClassifier(random_state=4)))\n",
    "\n",
    "\n",
    "for name, model in models:\n",
    "    print(f'{name}: {train_model_k_fold(model, accuracy_score)}')\n",
    "    ps.plot_learning_curve(model, name, X, y_raw, cv=k_fold_indexes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree\n",
    "`accuracy = 0.60`\n",
    "\n",
    "Learning Curve show us that model extremelly overfits the data, the gap between the curves show this, so the model has high variance and acquiring more data could help this overfits behavior. But, decision trees tend to overfit a lot, even more when we don't do feature selection, like we didn't.\n",
    "\n",
    "#### Logistic Regression\n",
    "`accuracy = 0.73`\n",
    "\n",
    "Learning Curve shows a very nice pattern: high score with close curves. We made a simple hyper parameter tunning at this stage (cheating, ok!) tunning C to regularize our 230+ features =D. Model answers very nice for our propose, one of the best choices.\n",
    "\n",
    "#### SVC\n",
    "`kernel: linear -> accuracy = 0.68`\n",
    "\n",
    "`kernel: poly -> accuracy = 0.65`\n",
    "\n",
    "`kernel: rbf -> accuracy = 0.72`\n",
    "\n",
    "`kernel: sigmoid -> accuracy = 0.72`\n",
    "\n",
    "At this point we made a simple hyper parameter tunning again (cheating), at my point of view, does no make sense choose an SVM model without choose a kernel, so we create 4 svc models, one for each basic kernel that has in sklearn.\n",
    "\n",
    "Linear kernel has high variance (overfit the data) and poly kernel has high bias (underfit the data). Sigmoid and rbf are both nice!\n",
    "\n",
    "#### Ensemble Methods\n",
    "`Gradient Boosting -> accuracy = 0.66`\n",
    "\n",
    "`Random Forest -> accuracy = 0.64`\n",
    "\n",
    "Both have good metrics, but is easy to see in learning curves that both are overfitted. Both cases, I think that more data could fix this overfitting problem, sadly we don't have more data, so let's move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Comments\n",
    "\n",
    "There are a bunch of other models that we could explore, but at this point, for our propose, those are enough to find a good solution.\n",
    "\n",
    "I decided to stress Linear Regression and SVC with sigmoid kernel to the next stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model - Second Scene - Stressing some algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this part we will find the best hyper parameters to chossed models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "scorer = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_lr = {'solver': ('newton-cg', 'sag', 'saga', 'lbfgs'),\n",
    "              'C': np.unique(np.geomspace(0.001, 1, num=15, dtype=float)),\n",
    "              'max_iter': np.unique(np.geomspace(50, 200, num=3, dtype=int))}\n",
    "\n",
    "lr = LogisticRegression(multi_class='auto')\n",
    "\n",
    "gd_model_lr = GridSearchCV(lr,\n",
    "                           parameters,\n",
    "                           n_jobs=4,\n",
    "                           cv=k_fold_indexes,\n",
    "                           iid=True,\n",
    "                           scoring=scorer,\n",
    "                           verbose=1)\n",
    "gd_model_lr.fit(X, y_raw)\n",
    "\n",
    "display(gd_model_lr.best_estimator_)\n",
    "gd_model_lr.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_scv = {'kernel': ('sigmoid', 'rbf'),\n",
    "                  'C': np.unique(np.geomspace(0.0001, 0.3, num=15, dtype=float)),\n",
    "                  'coef0': np.unique(np.geomspace(0.001, 2, num=5, dtype=float)),\n",
    "                  'class_weight': (None,\n",
    "                                   'balanced',\n",
    "                                   {'win_a': (354/(354-151)), 'win_b': (354/(354-151)), 'draw': (354/(354-52))}),\n",
    "                  'gamma': list(np.unique(np.geomspace(0.0001, 10, num=15, dtype=float))) + ['scale'],\n",
    "                  'tol': np.unique(np.geomspace(0.0001, 10, num=6, dtype=float))}\n",
    "\n",
    "\n",
    "svc = SVC(kernel='linear', decision_function_shape='ovo', random_state=4)\n",
    "\n",
    "gd_model_svc = GridSearchCV(svc,\n",
    "                            parameters_scv,\n",
    "                            n_jobs=8,\n",
    "                            cv=k_fold_indexes,\n",
    "                            iid=True,\n",
    "                            scoring=scorer,\n",
    "                            verbose=1)\n",
    "gd_model_svc.fit(X, y_raw)\n",
    "\n",
    "\n",
    "display(gd_model_svc.best_estimator_)\n",
    "gd_model_svc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gd_model_svc.best_estimator_\n",
    "print(f'Accuracy error: {train_model_k_fold(m, accuracy_score)}')\n",
    "ps.plot_learning_curve(m, 'Final Model - Learning Curve', X, y_raw, cv=k_fold_indexes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance based on decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(random_state=4)\n",
    "dt_model.fit(X, y_raw)\n",
    "\n",
    "feature_importance = [(f, importance) for f, importance in zip(X.columns, dt_model.feature_importances_)]\n",
    "\n",
    "relevant_features = [f for f, i in feature_importance if i>0]\n",
    "best_features = [f for f, i in feature_importance if i>0.01]\n",
    "best_features\n",
    "# relevant_features = [f, importance for f, importance in sorted(feature_importance, key=lambda x: x[1], reverse=True)]\n",
    "\n",
    "# for f, importance in sorted(feature_importance, key=lambda x: x[1], reverse=True):\n",
    "#     if importance > 0:\n",
    "#         print(f, importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "labels=['win_a', 'draw', 'win_b']\n",
    "cm1=confusion_matrix(y_test, pred, labels=labels)\n",
    "cm2=confusion_matrix(y_test, pred, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plots as ps\n",
    "\n",
    "from importlib import reload\n",
    "reload(ps)\n",
    "\n",
    "ps.print_confusion_matrixes([cm1, cm2, cm2, cm2, cm2], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
